{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lưu ý. Tự triển khai mô hình sau đấy so sánh kết quả với mô hình chạy bằng scikit learn. Chú ý chỉnh sửa đường dẫn đến thư mục dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\hoang tu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.14.0)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in c:\\users\\hoang tu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scipy) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T13:21:47.427082Z",
     "start_time": "2024-07-27T13:21:45.515394Z"
    }
   },
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T13:21:47.433066Z",
     "start_time": "2024-07-27T13:21:47.428100Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import coo_matrix # for sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xác định ngôn ngữ "
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('C:/HoangTu/Programing/Python/MachineLearningModelImplement/data/naivebayes/lang.csv')\n",
    "X = data['Text']\n",
    "y = data['language']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T13:21:47.623964Z",
     "start_time": "2024-07-27T13:21:47.434084Z"
    }
   },
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T13:21:49.639209Z",
     "start_time": "2024-07-27T13:21:47.624990Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.9575757575757575"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=32)\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris data\n",
    "\n",
    "Phân loại dữ liệu trên dự liệu hoa Iris:"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load data from sklearn\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "Y = data.target"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T13:21:49.647010Z",
     "start_time": "2024-07-27T13:21:49.640234Z"
    }
   },
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T13:21:49.670354Z",
     "start_time": "2024-07-27T13:21:49.648046Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.9777777777777777"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=32)\n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Xác định thư spam\n",
    "\n",
    "* Dữ liệu là tập con của bộ dữ liệu có tại link: [dataset](https://metatext.io/datasets/ling-spam-dataset). Tập dữ liệu này bao gồm tổng cộng 960 emails tiếng Anh, được tách thành tập training và test theo tỉ lệ 700:260, 50% trong mỗi tập là các spam emails.\n",
    "* Bộ dữ liệu đã được tiền xử lý theo các bước:\n",
    "\n",
    "* Loại bỏ stop words: Những từ xuất hiện thường xuyên như ‘and’, ‘the’, ‘of’, ... được loại bỏ.\n",
    "* Lemmatization: Những từ có cùng ‘gốc’ được đưa về cùng loại. Ví dụ, ‘include’, ‘includes’, ‘included’ đều được đưa chung về ‘include’. Tất cả các từ cũng đã được\n",
    "đưa về dạng ký tự thường (không phải HOA).\n",
    "* Loại bỏ non-words: Số, dấu câu, ký tự ‘tabs’, ký tự ‘xuống dòng’ đã được loại bỏ.\n",
    "\n",
    "* Bộ dữ liệu sau xử lý có trong link sau:\n",
    "[ex6DataPrepared.zip](http://openclassroom.stanford.edu/MainFolder/courses/MachineLearning/exercises/ex6materials/) - Tải file ex6DataPrepared.zip\n",
    "\n",
    "* Các tệp sau khi giải nén:\n",
    "    * test-features.txt\n",
    "    * test-labels.txt\n",
    "    * train-features-50.txt\n",
    "    * train-features-100.txt\n",
    "    * train-features-400.txt\n",
    "    * train-features.txt\n",
    "    * train-labels-50.txt\n",
    "    * train-labels-100.txt\n",
    "    * train-labels-400.txt\n",
    "    * train-labels.txt\n",
    "\n",
    "Tương ứng với các file chứa dữ liệu của tập training và tập test. File train-features-50.txt chứ dữ liệu của tập training thu gọn với chỉ có tổng cộng 50 training emails. Mỗi file *labels*.txt chứa nhiều dòng, mỗi dòng là một ký tự 0 hoặc 1 thể hiện email là non-spam hoặc spam. Mỗi file *features*.txt chứa nhiều dòng, mỗi dòng có 3 số, ví dụ:\n",
    "\n",
    "```\n",
    "1 564 1\n",
    "1 19 2\n",
    "```\n",
    "\n",
    "\n",
    "Trong đó số đầu tiên là chỉ số của email, bắt đầu từ 1; số thứ hai là thứ tự của từ trong từ điển (tổng cộng 2500 từ); số thứ ba là số lượng của từ đó trong email đang xét. Dòng đầu tiên nói rằng trong email thứ nhất, từ thứ 564 trong từ điển xuất hiện 1 lần. Cách lưu dữ liệu như thế này giúp tiết kiệm bộ nhớ vì 1 email thường không chứa hết tất cả các từ trong từ điển mà chỉ chứa một lượng nhỏ, ta chỉ cần lưu các giá trị khác không. Nếu ta biểu diễn feature vector của mỗi email là một vector hàng có độ dài bằng độ dài từ điển (2500) thì dòng thứ nhất nói rằng thành phần thứ 564 của vector này bằng 1. Tương tự, thành phần thứ 19 của vector này bằng 1. Nếu không xuất hiện, các thành phần khác được mặc định bằng 0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Tham khảo cách đọc dữ liệu từ file như sau: Lưu ý tự nghiên cứu cách chạy xử lý đến khi không tìm được cách mới xem cách đọc file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# data path and file name\n",
    "path = 'C:/HoangTu/Programing/Python/MachineLearningModelImplement/data/naivebayes/email/'\n",
    "train_data_fn = 'train-features.txt'\n",
    "test_data_fn = 'test-features.txt'\n",
    "train_label_fn = 'train-labels.txt'\n",
    "test_label_fn = 'test-labels.txt'\n",
    "n_words = 2500\n",
    "\n",
    "def read_data(data_fn, label_fn):\n",
    "    ## read label_fn\n",
    "    with open(path + label_fn) as f:\n",
    "        content = f.readlines()\n",
    "    label = [int(x.strip()) for x in content]\n",
    "    \n",
    "    ## read data_fn\n",
    "    with open(path + data_fn) as f:\n",
    "        content = f.readlines()\n",
    "    # remove '\\n' at the end of each line\n",
    "    content = [x.strip() for x in content]\n",
    "    \n",
    "    dat = np.zeros((len(content), 3), dtype = int)\n",
    "    \n",
    "    for i, line in enumerate(content):\n",
    "        a = line.split(' ')\n",
    "        dat[i, :] = np.array([int(a[0]), int(a[1]), int(a[2])])\n",
    "    \n",
    "    # remember to -1 at coordinate since we're in Python\n",
    "    # check this: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html\n",
    "    # for more information about coo_matrix function\n",
    "    data = coo_matrix((dat[:, 2], (dat[:, 0] - 1, dat[:, 1] - 1)), shape=(len(label), n_words))\n",
    "    return (data, label)\n",
    "\n",
    "(train_data, train_label) = read_data(train_data_fn, train_label_fn)\n",
    "(test_data, test_label) = read_data(test_data_fn, test_label_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-27T13:21:49.896344Z",
     "start_time": "2024-07-27T13:21:49.671372Z"
    }
   },
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
