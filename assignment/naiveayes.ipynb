{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lưu ý. Tự triển khai mô hình sau đấy so sánh kết quả với mô hình chạy bằng scikit learn. Chú ý chỉnh sửa đường dẫn đến thư mục dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T02:38:57.453835Z",
     "start_time": "2024-07-28T02:38:57.443957Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import coo_matrix # for sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xác định ngôn ngữ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T02:39:01.112501Z",
     "start_time": "2024-07-28T02:38:57.457636Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('C:/HoangTu/Programing/Python/MachineLearningModelImplement/data/naivebayes/lang.csv')\n",
    "X = data['Text']\n",
    "y = data['language']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(X)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Train the Naive Bayes model using the training data.\n",
    "        \n",
    "        Parameters:\n",
    "        X : numpy array, shape (n_samples, n_features)\n",
    "            Feature matrix of the training data.\n",
    "        y : numpy array, shape (n_samples,)\n",
    "            Array containing labels of the training data.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Initialize variables to store class information and feature counts\n",
    "        self.classes = np.unique(y)  # Unique classes in the target variable y\n",
    "        self.class_count = np.zeros(len(self.classes), dtype=np.float64)  # Count of each class\n",
    "\n",
    "        self.feature_count = np.zeros((len(self.classes), X.shape[1]), dtype=np.float64)  # Count of each feature for each class\n",
    "        self.feature_prob = np.zeros((len(self.classes), X.shape[1]), dtype=np.float64)  # Probability of each feature for each class\n",
    "\n",
    "        # Calculate class counts and feature counts        \n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]  # Subset of X where y == c (instances of class c)\n",
    "            self.class_count[idx] = X_c.shape[0]  # Number of instances of class c\n",
    "            self.feature_count[idx, :] = X_c.sum(axis=0)  # Sum of all features for instances of class c\n",
    "\n",
    "        # Calculate feature probabilities using Laplace smoothing\n",
    "        self.feature_prob = (self.feature_count + 1) / (self.class_count[:, None] + X.shape[1])\n",
    "\n",
    "        # Calculate class probabilities    \n",
    "        self.class_prob = self.class_count / X.shape[0]  # Prior probability of each class\n",
    "        return self\n",
    "\n",
    "    def _predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class of a feature sample x.\n",
    "\n",
    "        Parameters:\n",
    "        x : numpy array, shape (n_features,)\n",
    "            Feature sample to predict the class.\n",
    "\n",
    "        Returns:\n",
    "        predicted_class : object\n",
    "            Predicted class for feature sample x.\n",
    "        \"\"\"\n",
    "        posteriors = []\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            prior = np.log(self.class_prob[idx])  # Logarithm of the prior probability of class c\n",
    "            likelihood = np.sum(x * np.log(self.feature_prob[idx]))  # Log-likelihood of instance x belonging to class c\n",
    "            posterior = prior + likelihood  # Posterior probability of class c given instance x\n",
    "            posteriors.append(posterior)\n",
    "        predicted_class = self.classes[np.argmax(posteriors)]  # Predicted class is the one with the highest posterior probability\n",
    "        return predicted_class\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Predict the class of feature samples in matrix X.\n",
    "\n",
    "        Parameters:\n",
    "        X : numpy array, shape (n_samples, n_features)\n",
    "            Feature matrix of samples to predict the class.\n",
    "\n",
    "        Returns:\n",
    "        predicted_classes : numpy array, shape (n_samples,)\n",
    "            Array containing predicted classes for each feature sample in X.\n",
    "        \"\"\"\n",
    "        predicted_classes = np.array([self._predict(x) for x in X])  # Predict class for each feature sample in X\n",
    "        return predicted_classes\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)\n",
    "accuracy_score(y_test, y_predict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = MultinomialNaiveBayes()\n",
    "model.fit(x_train,y_train)\n",
    "y_predict = model.predict(x_test)\n",
    "accuracy_score(y_test, y_predict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Iris data\n",
    "\n",
    "Phân loại dữ liệu trên dự liệu hoa Iris:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load data from sklearn\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "Y = data.target"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit Gaussian Naive Bayes model to training data.\n",
    "\n",
    "        Parameters:\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Initialize parameters\n",
    "        self.classes = np.unique(y)\n",
    "        self.mean = np.zeros((len(self.classes), X.shape[1]), dtype=np.float64)\n",
    "        self.var = np.zeros((len(self.classes), X.shape[1]), dtype=np.float64)\n",
    "        self.priors = np.zeros(len(self.classes), dtype=np.float64)\n",
    "\n",
    "        # Calculate mean, variance, and class priors for each class\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.mean[idx, :] = X_c.mean(axis=0)\n",
    "            self.var[idx, :] = X_c.var(axis=0)\n",
    "            self.priors[idx] = X_c.shape[0] / X.shape[0]\n",
    "\n",
    "    def _pdf(self, class_idx, x):\n",
    "        \"\"\"\n",
    "        Compute the probability density function of Gaussian distribution.\n",
    "\n",
    "        Parameters:\n",
    "        class_idx : int\n",
    "            Index of the class.\n",
    "        x : array-like of shape (n_features,)\n",
    "            Input data point.\n",
    "\n",
    "        Returns:\n",
    "        pdf : array-like of shape (n_features,)\n",
    "            Probability density function values.\n",
    "        \"\"\"\n",
    "        mean = self.mean[class_idx]\n",
    "        var = self.var[class_idx]\n",
    "        numerator = np.exp(- (x - mean)**2 / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator / denominator\n",
    "\n",
    "    def _predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the class label for a single sample.\n",
    "\n",
    "        Parameters:\n",
    "        x : array-like of shape (n_features,)\n",
    "            Input data point.\n",
    "\n",
    "        Returns:\n",
    "        predicted_class : int\n",
    "            Predicted class label.\n",
    "        \"\"\"\n",
    "        posteriors = []\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            prior = np.log(self.priors[idx])\n",
    "            posterior = np.sum(np.log(self._pdf(idx, x)))\n",
    "            posterior = prior + posterior\n",
    "            posteriors.append(posterior)\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for multiple samples.\n",
    "\n",
    "        Parameters:\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input data.\n",
    "\n",
    "        Returns:\n",
    "        predicted_classes : array-like of shape (n_samples,)\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        return np.array([self._predict(x) for x in X])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)\n",
    "accuracy_score(y_test, y_predict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = GaussianNaiveBayes()\n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)\n",
    "accuracy_score(y_test, y_predict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Xác định thư spam\n",
    "\n",
    "* Dữ liệu là tập con của bộ dữ liệu có tại link: [dataset](https://metatext.io/datasets/ling-spam-dataset). Tập dữ liệu này bao gồm tổng cộng 960 emails tiếng Anh, được tách thành tập training và test theo tỉ lệ 700:260, 50% trong mỗi tập là các spam emails.\n",
    "* Bộ dữ liệu đã được tiền xử lý theo các bước:\n",
    "\n",
    "* Loại bỏ stop words: Những từ xuất hiện thường xuyên như ‘and’, ‘the’, ‘of’, ... được loại bỏ.\n",
    "* Lemmatization: Những từ có cùng ‘gốc’ được đưa về cùng loại. Ví dụ, ‘include’, ‘includes’, ‘included’ đều được đưa chung về ‘include’. Tất cả các từ cũng đã được\n",
    "đưa về dạng ký tự thường (không phải HOA).\n",
    "* Loại bỏ non-words: Số, dấu câu, ký tự ‘tabs’, ký tự ‘xuống dòng’ đã được loại bỏ.\n",
    "\n",
    "* Bộ dữ liệu sau xử lý có trong link sau:\n",
    "[ex6DataPrepared.zip](http://openclassroom.stanford.edu/MainFolder/courses/MachineLearning/exercises/ex6materials/) - Tải file ex6DataPrepared.zip\n",
    "\n",
    "* Các tệp sau khi giải nén:\n",
    "    * test-features.txt\n",
    "    * test-labels.txt\n",
    "    * train-features-50.txt\n",
    "    * train-features-100.txt\n",
    "    * train-features-400.txt\n",
    "    * train-features.txt\n",
    "    * train-labels-50.txt\n",
    "    * train-labels-100.txt\n",
    "    * train-labels-400.txt\n",
    "    * train-labels.txt\n",
    "\n",
    "Tương ứng với các file chứa dữ liệu của tập training và tập test. File train-features-50.txt chứ dữ liệu của tập training thu gọn với chỉ có tổng cộng 50 training emails. Mỗi file *labels*.txt chứa nhiều dòng, mỗi dòng là một ký tự 0 hoặc 1 thể hiện email là non-spam hoặc spam. Mỗi file *features*.txt chứa nhiều dòng, mỗi dòng có 3 số, ví dụ:\n",
    "\n",
    "```\n",
    "1 564 1\n",
    "1 19 2\n",
    "```\n",
    "\n",
    "\n",
    "Trong đó số đầu tiên là chỉ số của email, bắt đầu từ 1; số thứ hai là thứ tự của từ trong từ điển (tổng cộng 2500 từ); số thứ ba là số lượng của từ đó trong email đang xét. Dòng đầu tiên nói rằng trong email thứ nhất, từ thứ 564 trong từ điển xuất hiện 1 lần. Cách lưu dữ liệu như thế này giúp tiết kiệm bộ nhớ vì 1 email thường không chứa hết tất cả các từ trong từ điển mà chỉ chứa một lượng nhỏ, ta chỉ cần lưu các giá trị khác không. Nếu ta biểu diễn feature vector của mỗi email là một vector hàng có độ dài bằng độ dài từ điển (2500) thì dòng thứ nhất nói rằng thành phần thứ 564 của vector này bằng 1. Tương tự, thành phần thứ 19 của vector này bằng 1. Nếu không xuất hiện, các thành phần khác được mặc định bằng 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> Tham khảo cách đọc dữ liệu từ file như sau: Lưu ý tự nghiên cứu cách chạy xử lý đến khi không tìm được cách mới xem cách đọc file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-28T02:39:02.044949Z",
     "start_time": "2024-07-28T02:39:01.634114Z"
    }
   },
   "outputs": [],
   "source": [
    "# data path and file name\n",
    "path = 'C:/HoangTu/Programing/Python/MachineLearningModelImplement/data/naivebayes/email/'\n",
    "train_data_fn = 'train-features.txt'\n",
    "test_data_fn = 'test-features.txt'\n",
    "train_label_fn = 'train-labels.txt'\n",
    "test_label_fn = 'test-labels.txt'\n",
    "n_words = 2500\n",
    "\n",
    "def read_data(data_fn, label_fn):\n",
    "    ## read label_fn\n",
    "    with open(path + label_fn) as f:\n",
    "        content = f.readlines()\n",
    "    label = [int(x.strip()) for x in content]\n",
    "    \n",
    "    ## read data_fn\n",
    "    with open(path + data_fn) as f:\n",
    "        content = f.readlines()\n",
    "    # remove '\\n' at the end of each line\n",
    "    content = [x.strip() for x in content]\n",
    "    \n",
    "    dat = np.zeros((len(content), 3), dtype = int)\n",
    "    \n",
    "    for i, line in enumerate(content):\n",
    "        a = line.split(' ')\n",
    "        dat[i, :] = np.array([int(a[0]), int(a[1]), int(a[2])])\n",
    "    \n",
    "    # remember to -1 at coordinate since we're in Python\n",
    "    # check this: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html\n",
    "    # for more information about coo_matrix function\n",
    "    data = coo_matrix((dat[:, 2], (dat[:, 0] - 1, dat[:, 1] - 1)), shape=(len(label), n_words))\n",
    "    return (data, label)\n",
    "\n",
    "(train_data, train_label) = read_data(train_data_fn, train_label_fn)\n",
    "(test_data, test_label) = read_data(test_data_fn, test_label_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
